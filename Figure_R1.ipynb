{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"%matplotlib inline\\n%load_ext nb_black\\nimport pandas as pd\\nimport numpy as np\\nimport random\\nimport itertools\\n\\nimport matplotlib.pylab as plt\\nfrom matplotlib.patches import Ellipse\\nimport matplotlib.transforms as transforms\\n\\nfrom scipy.io import loadmat, savemat\\nfrom scipy.stats import f_oneway\\nfrom IPython.core.debugger import set_trace\\n\\nfrom sklearn.feature_selection import mutual_info_regression\\n\\n\\\"\\\"\\\"\\nmonkey1\\n59 sessions\\n7 variables\\uff1a(2380, 96)- spike count matrix, (2380,), (1, 2380), (2380,), (1, 2380)-orientation, (1, 1), (1, 2380)\\n\\nmonkey2\\n71 sessions\\n\\n\\\"\\\"\\\"\\n\\n\\n\\\"\\\"\\\"sample to make the two monkey has comparable data size\\\"\\\"\\\"\\n\\n\\ndef sample_sessions(monkey_num, save=False):\\n    data = loadmat(\\\"monkey\\\" + \\\"1\\\" + \\\".mat\\\")[\\\"monkey\\\" + \\\"1\\\"][0]\\n    result = []\\n    for i in data:\\n        mask = i[0][0][-1][0] == max(i[0][0][-1][0])\\n        result.append([i[0][0][0][mask], i[0][0][-3][0][mask].reshape(1, -1)])\\n    samples = [\\n        filter_data_sample(result[session_num])[0].shape[0]\\n        for session_num in range(len(result))\\n    ]\\n\\n    \\\"sample from normal distribution\\\"\\n    while True:\\n        norm_samples = np.random.normal(\\n            loc=np.mean(samples), scale=np.std(samples), size=len(get_data(monkey_num))\\n        ).round(0)\\n        if (norm_samples < 0).sum() == 0:\\n            norm_samples.sort()\\n            break\\n\\n    \\\"sampling\\\"\\n    data = loadmat(\\\"monkey\\\" + monkey_num + \\\".mat\\\")[\\\"monkey\\\" + monkey_num][0]\\n    result = []\\n    for i in data:\\n        mask = i[0][0][-1][0] == max(i[0][0][-1][0])\\n        result.append([i[0][0][0][mask], i[0][0][-3][0][mask].reshape(1, -1)])\\n    sample_s = pd.DataFrame(\\n        [\\n            filter_data_sample(result[session_num])[0].shape[0]\\n            for session_num in range(len(result))\\n        ],\\n        columns=[\\\"data2_samples\\\"],\\n    )\\n    result = (\\n        sample_s.sort_values(by=\\\"data2_samples\\\")\\n        .assign(ideal_samples=norm_samples)\\n        .apply(lambda x: min(x.values), 1)\\n    )\\n    return result.sort_index()\\n\\n\\ndef sample_from_one_session(session_data, sample_number, save=False):\\n    sort_nums = np.arange(session_data[0].shape[0])\\n    random.shuffle(sort_nums)\\n    if save:\\n        return [\\n            session_data[0][sort_nums[: int(sample_number)]],\\n            session_data[1][sort_nums[: int(sample_number)]],\\n            session_data[2][:, sort_nums[: int(sample_number)]],\\n            session_data[3][sort_nums[: int(sample_number)]],\\n            session_data[4][:, sort_nums[: int(sample_number)]],\\n            session_data[5],\\n            session_data[6][:, sort_nums[: int(sample_number)]],\\n        ]\\n    else:\\n        return [\\n            session_data[0][sort_nums[: int(sample_number)]],\\n            session_data[1][:, sort_nums[: int(sample_number)]],\\n        ]\\n\\n\\ndef get_data(monkey_index, bz=3, if_sample=False, filter_neurons=False):\\n    \\\"\\\"\\\"\\n    i: seesion index, 0-counts_matrix, -3 orientation\\n    \\\"\\\"\\\"\\n    data = loadmat(\\\"monkey\\\" + monkey_index + \\\".mat\\\")[\\\"monkey\\\" + monkey_index][0]\\n    result = []\\n    for i in data:\\n        mask = i[0][0][-1][0] == max(i[0][0][-1][0])\\n        result.append([i[0][0][0][mask], i[0][0][-3][0][mask].reshape(1, -1)])\\n    if if_sample:\\n        sample_numbers = sample_sessions(monkey_index)\\n        result = [\\n            sample_from_one_session(result[i], sample_numbers[i])\\n            for i in range(len(sample_numbers))\\n        ]\\n    if filter_neurons:\\n        result_final = []\\n        for session_num in range(len(result)):\\n            data_sample = filter_data_sample(result[session_num])  # select one session\\n            pvalues = f_oneway(*list(split_data(data_sample, bin_size=bz).values()))[1]\\n            result_final.append([data_sample[0][:, pvalues < 0.05], data_sample[1]])\\n        result = result_final\\n    return result\\n\\n\\ndef get_data_save(monkey_index, bz=3, if_sample=False, filter_neurons=False):\\n    \\\"\\\"\\\"\\n    i: seesion index, 0-counts_matrix, -3 orientation\\n    \\\"\\\"\\\"\\n    data = loadmat(\\\"monkey\\\" + monkey_index + \\\".mat\\\")[\\\"monkey\\\" + monkey_index][0]\\n    result = []\\n    for i in data:\\n        mask = i[0][0][-1][0] == max(i[0][0][-1][0])\\n        result.append(\\n            [\\n                i[0][0][0][mask],\\n                i[0][0][1][mask],\\n                i[0][0][2][:, mask],\\n                i[0][0][3][mask],\\n                i[0][0][-3][:, mask],\\n                i[0][0][-2],\\n                i[0][0][-1][:, mask],\\n            ]\\n        )\\n    if if_sample:\\n        sample_numbers = sample_sessions(monkey_index, save=True)\\n        result = [\\n            sample_from_one_session(result[i], sample_numbers[i], save=True)\\n            for i in range(len(sample_numbers))\\n        ]\\n    if filter_neurons:\\n        result_final = []\\n        for session_num in range(len(result)):\\n            data_sample = filter_data_sample(\\n                result[session_num], save=True\\n            )  # select one session\\n            f_params = [\\n                i[0]\\n                for i in split_data(data_sample, bin_size=bz, save=True).values()\\n                if len(i[0]) > 0\\n            ]\\n            pvalues = f_oneway(*f_params)[1]\\n            result_final.append(\\n                [\\n                    data_sample[0][:, pvalues < 0.05],\\n                    data_sample[1],\\n                    data_sample[2],\\n                    data_sample[3],\\n                    data_sample[4],\\n                    data_sample[5],\\n                    data_sample[6],\\n                ]\\n            )\\n        result = result_final\\n    return result\\n\\n\\ndef split_data(data_sample, bin_size, limit=0, save=False):\\n    #     \\\"Counts_matrix\\\",\\n    #                             \\\"orientation\\\",\\n    #                             \\\"stimulus_class\\\",\\n    #                             \\\"trial_num\\\",\\n    #                             \\\"selected_class\\\",\\n    #                             \\\"session_number\\\",\\n    #                             \\\"contrast\\\"\\n    if save:\\n        orientation = data_sample[-3].ravel()\\n    else:\\n        orientation = data_sample[1].ravel()\\n    groups = pd.cut(\\n        pd.Series(orientation),\\n        range(\\n            np.floor(orientation.min()).astype(int) - 1,\\n            np.ceil(orientation.max()).astype(int) + bin_size,\\n            bin_size,\\n        ),\\n    )\\n    df = pd.concat(\\n        [\\n            pd.DataFrame(data_sample[0]),\\n            pd.Series(orientation).rename(\\\"orientation\\\"),\\n            groups.rename(\\\"group\\\"),\\n        ],\\n        1,\\n    )\\n    if save:\\n        df = pd.concat(\\n            [\\n                df,\\n                pd.Series(data_sample[1]).rename(\\\"stimulus_class\\\"),\\n                pd.Series(data_sample[2].ravel()).rename(\\\"trial_num\\\"),\\n                pd.Series(data_sample[3]).rename(\\\"selected_class\\\"),\\n                pd.Series(list(data_sample[5][0]) * len(data_sample[1])).rename(\\n                    \\\"session_number\\\"\\n                ),\\n                pd.Series(data_sample[6].ravel()).rename(\\\"contrast\\\"),\\n            ],\\n            1,\\n        )\\n    return dict(\\n        df.groupby(\\\"group\\\")\\n        .apply(\\n            lambda x: [x.drop(\\\"group\\\", 1).values[:, :]]\\n            #             + [\\n            #                 each.ravel()\\n            #                 for each in np.split(x.drop(\\\"group\\\", 1).values[:, -6:], range(1, 6), 1)\\n            #             ]\\n            #             if x.shape[0] >= limit\\n            #             else np.nan\\n        )\\n        .dropna()\\n    )\\n\\n\\ndef bootstrap_sample(x, size_per_time, exp_num=1, times=100):\\n    sample = [\\n        (np.random.choice(x, size_per_time) ** exp_num).mean() for _ in range(times)\\n    ]\\n    return [np.mean(sample), np.std(sample)]\\n\\n\\ndef plot_ten_neurons(dfs, select_p, exp_num=1, legend_on=True):\\n    x = [i.mid for i in list(dfs.keys()) if dfs[i].shape[0] > 0]\\n    for idx in select_p:\\n        y, yerr = zip(\\n            *[\\n                bootstrap_sample(i[:, idx], i.shape[0], exp_num)\\n                for i in list(dfs.values())\\n            ]\\n        )\\n        if list(dfs.values())[0].shape[1] < 100:\\n            if exp_num > 1:\\n                plt.plot(x, y)\\n                plt.fill_between(\\n                    x,\\n                    np.array(y) + np.array(yerr),\\n                    np.array(y) - np.array(yerr),\\n                    alpha=0.2,\\n                    #                     capsize=3,\\n                    label=r\\\"$<r^\\\" + str(exp_num) + \\\"_{\\\" + str(idx + 1) + \\\"}|s>$\\\",\\n                )\\n            else:\\n                plt.plot(\\n                    x, y, label=r\\\"$<r^\\\" + str(exp_num) + \\\"_{\\\" + str(idx + 1) + \\\"}|s>$\\\",\\n                )\\n                plt.fill_between(\\n                    x,\\n                    np.array(y) + np.array(yerr),\\n                    np.array(y) - np.array(yerr),\\n                    alpha=0.2,\\n                    #                     capsize=3,\\n                    #                     label=r\\\"$<r_{\\\" + str(idx + 1) + \\\"}|s>$\\\",\\n                )\\n        else:\\n            triu_index = np.triu_indices(96)\\n            plt.plot(x, y)\\n            plt.fill_between(\\n                x,\\n                np.array(y) + np.array(yerr),\\n                np.array(y) - np.array(yerr),\\n                alpha=0.2,\\n                #                     capsize=3,\\n                label=r\\\"$<r_{\\\"\\n                + str(triu_index[0][idx] + 1)\\n                + \\\"} r_{\\\"\\n                + str(triu_index[1][idx] + 1)\\n                + \\\"}|s>$\\\",\\n            )\\n    if exp_num == 1 and legend_on == True:\\n        plt.legend(frameon=False, ncol=2)\\n    else:\\n        plt.legend().remove()\\n    plt.xlabel(\\\"orientation\\\")\\n\\n\\n#     plt.ylabel(\\\"average stimulus\\\")\\n\\n\\ndef normalize_across_trial(m, get_first=False):\\n    if get_first:\\n        return m[0] - m[0].mean(0)\\n    else:\\n        return m - m.mean(0)\\n\\n\\ndef eigsorted(cov):\\n    vals, vecs = np.linalg.eigh(cov)\\n    order = vals.argsort()[::-1]\\n    return vals[order], vecs[:, order]\\n\\n\\ndef draw_ellipse(x, y, ax, label, color):\\n    nstd = 2\\n    cov = np.cov(x, y)\\n    vals, vecs = eigsorted(cov)\\n    theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\\n    w, h = 2 * nstd * np.sqrt(vals)\\n    sc = plt.scatter(x, y, label=label, s=3, c=color)\\n    ell = Ellipse(\\n        xy=(np.mean(x), np.mean(y)),\\n        width=w,\\n        height=h,\\n        angle=theta,\\n        #         color=sc.get_facecolors()[0].tolist(),\\n        color=color,\\n        linestyle=\\\"-\\\",\\n    )\\n\\n    ell.set_facecolor(\\\"none\\\")\\n    xmax, ymax = ell.get_patch_transform().transform(ell.get_path().vertices).max(0)\\n    xmin, ymin = ell.get_patch_transform().transform(ell.get_path().vertices).min(0)\\n    ax.add_artist(ell)\\n    ax.axes.xaxis.set_major_locator(plt.MaxNLocator(4))\\n    ax.axes.yaxis.set_major_locator(plt.MaxNLocator(4))\\n    return xmin, ymin, xmax, ymax\\n\\n\\ndef filter_data_sample(data_sample, save=False):\\n    if save:\\n        mask = np.array(\\n            [True if i >= 210 and i <= 330 else False for i in data_sample[-3][0]],\\n            dtype=bool,\\n        ) & np.array((data_sample[0] > 200).sum(1) == 0, dtype=bool)\\n        data_sample[0] = data_sample[0][mask],\\n        data_sample[1]= data_sample[1][mask],\\n        data_sample[2] = data_sample[2][:, mask],\\n        data_sample[3] = data_sample[3][mask],\\n        data_sample[-3]=data_sample[-3][:, mask],\\n        data_sample[-2] = data_sample[-2],\\n        data_sample[-1] = data_sample[-1][:, mask],\\n    else:\\n        mask = np.array(\\n            [True if i >= 210 and i <= 330 else False for i in data_sample[1][0]],\\n            dtype=bool,\\n        ) & np.array((data_sample[0] > 200).sum(1) == 0, dtype=bool)\\n        data_sample[0] = data_sample[0][mask, :]\\n        data_sample[1] = data_sample[1][:, mask]\\n    return data_sample\\n\\n\\ndef get_index_list(pair_neuron_list):\\n    l = []\\n    for i in pair_neuron_list:\\n        ref_df = pd.DataFrame(np.concatenate(np.triu_indices(96)).reshape(2, -1)).T\\n        l.append(ref_df[(ref_df[0] == i[0]) & (ref_df[1] == i[1])].index[0])\\n    return l\\n\\n\\ndef sensivity(x, y):\\n    return (x.mean(0) - y.mean(0)) / np.where(\\n        ((np.var(x, 0) + np.var(y, 0)) / 2) ** 0.5 == 0,\\n        np.nan,\\n        ((np.var(x, 0) + np.var(y, 0)) / 2) ** 0.5,\\n    )\\n\\n\\n\\\"\\\"\\\"\\ncalculate sensitivity\\n\\\"\\\"\\\"\\n\\n\\ndef all_sensitivity(data, savename, monkey_num, bz=3, cate=\\\"linear\\\"):\\n    sens_total = []\\n    for d in data:\\n        data_sample = filter_data_sample(d)\\n        dfs = split_data(data_sample, bin_size=bz)\\n        if cate in [\\\"cross\\\", \\\"square\\\"]:\\n            norm_dfs = [normalize_across_trial(i) for i in list(dfs.values())]\\n            r_ij = np.expand_dims(np.concatenate(norm_dfs), 2) * np.expand_dims(\\n                np.concatenate(norm_dfs), 1\\n            )\\n            if cate == \\\"cross\\\":\\n                sep_index = np.triu_indices(96, k=1)\\n            else:\\n                sep_index = np.diag_indices(96)\\n            r_ij_flat = np.vstack([r_ij[i][sep_index] for i in range(r_ij.shape[0])])\\n            data_sample = filter_data_sample([r_ij_flat, data_sample[1]])\\n            dfs = split_data(data_sample, bin_size=bz)\\n        value_list = list(dfs.values())\\n\\n        sens_total.extend(\\n            np.concatenate(\\n                [\\n                    sensivity(value_list[i + 1], value_list[i])\\n                    for i in range(len(dfs) - 1)\\n                    if value_list[i].shape[0] >= 10 and value_list[i + 1].shape[0] >= 10\\n                ]\\n            )\\n        )\\n\\n    pd.Series(sens_total).hist(bins=np.linspace(-2, 2, 100))\\n\\n    plt.title(\\\"monkey \\\" + monkey_num + \\\" all sessions, \\\" + str(cate))\\n    plt.grid(False)\\n    plt.savefig(savename + \\\"_filter.pdf\\\")\\n\\n\\ndef single_sensitivity(data, session_num, savename, bz=3, cate=\\\"linear\\\", save=True):\\n    sens_total = []\\n    data_sample = filter_data_sample(data[session_num])\\n    dfs = split_data(data_sample, bin_size=bz)\\n    if cate in [\\\"cross\\\", \\\"square\\\"]:\\n        norm_dfs = [normalize_across_trial(i) for i in list(dfs.values())]\\n        r_ij = np.expand_dims(np.concatenate(norm_dfs), 2) * np.expand_dims(\\n            np.concatenate(norm_dfs), 1\\n        )\\n        if cate == \\\"cross\\\":\\n            sep_index = np.triu_indices(96, k=1)\\n        else:\\n            sep_index = np.diag_indices(96)\\n        r_ij_flat = np.vstack([r_ij[i][sep_index] for i in range(r_ij.shape[0])])\\n        data_sample = filter_data_sample([r_ij_flat, data_sample[1]])\\n        dfs = split_data(data_sample, bin_size=bz)\\n    value_list = list(dfs.values())\\n    sens_total.extend(\\n        [\\n            [\\n                sensivity(value_list[i + 1], value_list[i]),\\n                list(dfs.keys())[i].mid,\\n                list(dfs.keys())[i + 1].mid,\\n            ]\\n            for i in range(len(dfs) - 1)\\n            if value_list[i].shape[0] >= 10 and value_list[i + 1].shape[0] >= 10\\n        ]\\n    )\\n    edge = int((len(sens_total) - 4) // 2)\\n    for s in sens_total[edge : edge + 4]:\\n        pd.Series(s[0]).hist(\\n            bins=np.linspace(-1, 1, 50),\\n            alpha=0.7,\\n            label=\\\"orientation \\\" + str(s[1]) + \\\" \\\" + str(s[2]),\\n        )\\n    plt.legend()\\n    plt.title(\\n        \\\"monkey \\\"\\n        + str(monkey_num)\\n        + \\\", session: \\\"\\n        + str(session_num)\\n        + \\\", bin size: \\\"\\n        + str(bz)\\n        + \\\", \\\"\\n        + str(cate)\\n    )\\n    plt.grid(False)\\n    if save:\\n        plt.savefig(savename + \\\"_filter.pdf\\\")\\n\\n\\ndef change_dtypes(aa):\\n    revise_dtypes = {\\n        \\\"Counts_matrix\\\": \\\"uint8\\\",\\n        \\\"orientation\\\": \\\"double\\\",\\n        \\\"stimulus_class\\\": \\\"<U1\\\",\\n        \\\"trial_num\\\": \\\"uint16\\\",\\n        \\\"selected_class\\\": \\\"<U1\\\",\\n        \\\"session_number\\\": \\\"int\\\",\\n        \\\"contrast\\\": \\\"double\\\",\\n    }\\n    for key, item in aa.items():\\n        if key == \\\"session_number\\\":\\n            aa[key] = item[0]\\n        elif key in [\\\"trial_num\\\", \\\"contrast\\\", \\\"orientation\\\"]:\\n            aa[key] = item.reshape(1, -1).astype(revise_dtypes[key])\\n        else:\\n            aa[key] = item.astype(revise_dtypes[key])\\n    return aa\\n\\n\\nparams = {\\n    \\\"legend.fontsize\\\": 14,\\n    \\\"legend.frameon\\\": False,\\n    \\\"ytick.labelsize\\\": 14,\\n    \\\"xtick.labelsize\\\": 14,\\n    \\\"figure.dpi\\\": 300,\\n    \\\"axes.labelsize\\\": 14,\\n    \\\"axes.titlesize\\\": 14,\\n    \\\"pdf.fonttype\\\": 42,\\n    \\\"font.sans-serif\\\": \\\"Myriad Pro\\\",\\n    \\\"font.family\\\": \\\"sans-serif\\\",\\n}\\nplt.rcParams.update(params)\\n\\n\\nmonkey_num = \\\"2\\\"\\nbz = 3  # bin size is 3\\nsession_num = 20\";\n",
       "                var nbb_formatted_code = \"%matplotlib inline\\n%load_ext nb_black\\nimport pandas as pd\\nimport numpy as np\\nimport random\\nimport itertools\\n\\nimport matplotlib.pylab as plt\\nfrom matplotlib.patches import Ellipse\\nimport matplotlib.transforms as transforms\\n\\nfrom scipy.io import loadmat, savemat\\nfrom scipy.stats import f_oneway\\nfrom IPython.core.debugger import set_trace\\n\\nfrom sklearn.feature_selection import mutual_info_regression\\n\\n\\\"\\\"\\\"\\nmonkey1\\n59 sessions\\n7 variables\\uff1a(2380, 96)- spike count matrix, (2380,), (1, 2380), (2380,), (1, 2380)-orientation, (1, 1), (1, 2380)\\n\\nmonkey2\\n71 sessions\\n\\n\\\"\\\"\\\"\\n\\n\\n\\\"\\\"\\\"sample to make the two monkey has comparable data size\\\"\\\"\\\"\\n\\n\\ndef sample_sessions(monkey_num, save=False):\\n    data = loadmat(\\\"monkey\\\" + \\\"1\\\" + \\\".mat\\\")[\\\"monkey\\\" + \\\"1\\\"][0]\\n    result = []\\n    for i in data:\\n        mask = i[0][0][-1][0] == max(i[0][0][-1][0])\\n        result.append([i[0][0][0][mask], i[0][0][-3][0][mask].reshape(1, -1)])\\n    samples = [\\n        filter_data_sample(result[session_num])[0].shape[0]\\n        for session_num in range(len(result))\\n    ]\\n\\n    \\\"sample from normal distribution\\\"\\n    while True:\\n        norm_samples = np.random.normal(\\n            loc=np.mean(samples), scale=np.std(samples), size=len(get_data(monkey_num))\\n        ).round(0)\\n        if (norm_samples < 0).sum() == 0:\\n            norm_samples.sort()\\n            break\\n\\n    \\\"sampling\\\"\\n    data = loadmat(\\\"monkey\\\" + monkey_num + \\\".mat\\\")[\\\"monkey\\\" + monkey_num][0]\\n    result = []\\n    for i in data:\\n        mask = i[0][0][-1][0] == max(i[0][0][-1][0])\\n        result.append([i[0][0][0][mask], i[0][0][-3][0][mask].reshape(1, -1)])\\n    sample_s = pd.DataFrame(\\n        [\\n            filter_data_sample(result[session_num])[0].shape[0]\\n            for session_num in range(len(result))\\n        ],\\n        columns=[\\\"data2_samples\\\"],\\n    )\\n    result = (\\n        sample_s.sort_values(by=\\\"data2_samples\\\")\\n        .assign(ideal_samples=norm_samples)\\n        .apply(lambda x: min(x.values), 1)\\n    )\\n    return result.sort_index()\\n\\n\\ndef sample_from_one_session(session_data, sample_number, save=False):\\n    sort_nums = np.arange(session_data[0].shape[0])\\n    random.shuffle(sort_nums)\\n    if save:\\n        return [\\n            session_data[0][sort_nums[: int(sample_number)]],\\n            session_data[1][sort_nums[: int(sample_number)]],\\n            session_data[2][:, sort_nums[: int(sample_number)]],\\n            session_data[3][sort_nums[: int(sample_number)]],\\n            session_data[4][:, sort_nums[: int(sample_number)]],\\n            session_data[5],\\n            session_data[6][:, sort_nums[: int(sample_number)]],\\n        ]\\n    else:\\n        return [\\n            session_data[0][sort_nums[: int(sample_number)]],\\n            session_data[1][:, sort_nums[: int(sample_number)]],\\n        ]\\n\\n\\ndef get_data(monkey_index, bz=3, if_sample=False, filter_neurons=False):\\n    \\\"\\\"\\\"\\n    i: seesion index, 0-counts_matrix, -3 orientation\\n    \\\"\\\"\\\"\\n    data = loadmat(\\\"monkey\\\" + monkey_index + \\\".mat\\\")[\\\"monkey\\\" + monkey_index][0]\\n    result = []\\n    for i in data:\\n        mask = i[0][0][-1][0] == max(i[0][0][-1][0])\\n        result.append([i[0][0][0][mask], i[0][0][-3][0][mask].reshape(1, -1)])\\n    if if_sample:\\n        sample_numbers = sample_sessions(monkey_index)\\n        result = [\\n            sample_from_one_session(result[i], sample_numbers[i])\\n            for i in range(len(sample_numbers))\\n        ]\\n    if filter_neurons:\\n        result_final = []\\n        for session_num in range(len(result)):\\n            data_sample = filter_data_sample(result[session_num])  # select one session\\n            pvalues = f_oneway(*list(split_data(data_sample, bin_size=bz).values()))[1]\\n            result_final.append([data_sample[0][:, pvalues < 0.05], data_sample[1]])\\n        result = result_final\\n    return result\\n\\n\\ndef get_data_save(monkey_index, bz=3, if_sample=False, filter_neurons=False):\\n    \\\"\\\"\\\"\\n    i: seesion index, 0-counts_matrix, -3 orientation\\n    \\\"\\\"\\\"\\n    data = loadmat(\\\"monkey\\\" + monkey_index + \\\".mat\\\")[\\\"monkey\\\" + monkey_index][0]\\n    result = []\\n    for i in data:\\n        mask = i[0][0][-1][0] == max(i[0][0][-1][0])\\n        result.append(\\n            [\\n                i[0][0][0][mask],\\n                i[0][0][1][mask],\\n                i[0][0][2][:, mask],\\n                i[0][0][3][mask],\\n                i[0][0][-3][:, mask],\\n                i[0][0][-2],\\n                i[0][0][-1][:, mask],\\n            ]\\n        )\\n    if if_sample:\\n        sample_numbers = sample_sessions(monkey_index, save=True)\\n        result = [\\n            sample_from_one_session(result[i], sample_numbers[i], save=True)\\n            for i in range(len(sample_numbers))\\n        ]\\n    if filter_neurons:\\n        result_final = []\\n        for session_num in range(len(result)):\\n            data_sample = filter_data_sample(\\n                result[session_num], save=True\\n            )  # select one session\\n            f_params = [\\n                i[0]\\n                for i in split_data(data_sample, bin_size=bz, save=True).values()\\n                if len(i[0]) > 0\\n            ]\\n            pvalues = f_oneway(*f_params)[1]\\n            result_final.append(\\n                [\\n                    data_sample[0][:, pvalues < 0.05],\\n                    data_sample[1],\\n                    data_sample[2],\\n                    data_sample[3],\\n                    data_sample[4],\\n                    data_sample[5],\\n                    data_sample[6],\\n                ]\\n            )\\n        result = result_final\\n    return result\\n\\n\\ndef split_data(data_sample, bin_size, limit=0, save=False):\\n    #     \\\"Counts_matrix\\\",\\n    #                             \\\"orientation\\\",\\n    #                             \\\"stimulus_class\\\",\\n    #                             \\\"trial_num\\\",\\n    #                             \\\"selected_class\\\",\\n    #                             \\\"session_number\\\",\\n    #                             \\\"contrast\\\"\\n    if save:\\n        orientation = data_sample[-3].ravel()\\n    else:\\n        orientation = data_sample[1].ravel()\\n    groups = pd.cut(\\n        pd.Series(orientation),\\n        range(\\n            np.floor(orientation.min()).astype(int) - 1,\\n            np.ceil(orientation.max()).astype(int) + bin_size,\\n            bin_size,\\n        ),\\n    )\\n    df = pd.concat(\\n        [\\n            pd.DataFrame(data_sample[0]),\\n            pd.Series(orientation).rename(\\\"orientation\\\"),\\n            groups.rename(\\\"group\\\"),\\n        ],\\n        1,\\n    )\\n    if save:\\n        df = pd.concat(\\n            [\\n                df,\\n                pd.Series(data_sample[1]).rename(\\\"stimulus_class\\\"),\\n                pd.Series(data_sample[2].ravel()).rename(\\\"trial_num\\\"),\\n                pd.Series(data_sample[3]).rename(\\\"selected_class\\\"),\\n                pd.Series(list(data_sample[5][0]) * len(data_sample[1])).rename(\\n                    \\\"session_number\\\"\\n                ),\\n                pd.Series(data_sample[6].ravel()).rename(\\\"contrast\\\"),\\n            ],\\n            1,\\n        )\\n    return dict(\\n        df.groupby(\\\"group\\\")\\n        .apply(\\n            lambda x: [x.drop(\\\"group\\\", 1).values[:, :]]\\n            #             + [\\n            #                 each.ravel()\\n            #                 for each in np.split(x.drop(\\\"group\\\", 1).values[:, -6:], range(1, 6), 1)\\n            #             ]\\n            #             if x.shape[0] >= limit\\n            #             else np.nan\\n        )\\n        .dropna()\\n    )\\n\\n\\ndef bootstrap_sample(x, size_per_time, exp_num=1, times=100):\\n    sample = [\\n        (np.random.choice(x, size_per_time) ** exp_num).mean() for _ in range(times)\\n    ]\\n    return [np.mean(sample), np.std(sample)]\\n\\n\\ndef plot_ten_neurons(dfs, select_p, exp_num=1, legend_on=True):\\n    x = [i.mid for i in list(dfs.keys()) if dfs[i].shape[0] > 0]\\n    for idx in select_p:\\n        y, yerr = zip(\\n            *[\\n                bootstrap_sample(i[:, idx], i.shape[0], exp_num)\\n                for i in list(dfs.values())\\n            ]\\n        )\\n        if list(dfs.values())[0].shape[1] < 100:\\n            if exp_num > 1:\\n                plt.plot(x, y)\\n                plt.fill_between(\\n                    x,\\n                    np.array(y) + np.array(yerr),\\n                    np.array(y) - np.array(yerr),\\n                    alpha=0.2,\\n                    #                     capsize=3,\\n                    label=r\\\"$<r^\\\" + str(exp_num) + \\\"_{\\\" + str(idx + 1) + \\\"}|s>$\\\",\\n                )\\n            else:\\n                plt.plot(\\n                    x, y, label=r\\\"$<r^\\\" + str(exp_num) + \\\"_{\\\" + str(idx + 1) + \\\"}|s>$\\\",\\n                )\\n                plt.fill_between(\\n                    x,\\n                    np.array(y) + np.array(yerr),\\n                    np.array(y) - np.array(yerr),\\n                    alpha=0.2,\\n                    #                     capsize=3,\\n                    #                     label=r\\\"$<r_{\\\" + str(idx + 1) + \\\"}|s>$\\\",\\n                )\\n        else:\\n            triu_index = np.triu_indices(96)\\n            plt.plot(x, y)\\n            plt.fill_between(\\n                x,\\n                np.array(y) + np.array(yerr),\\n                np.array(y) - np.array(yerr),\\n                alpha=0.2,\\n                #                     capsize=3,\\n                label=r\\\"$<r_{\\\"\\n                + str(triu_index[0][idx] + 1)\\n                + \\\"} r_{\\\"\\n                + str(triu_index[1][idx] + 1)\\n                + \\\"}|s>$\\\",\\n            )\\n    if exp_num == 1 and legend_on == True:\\n        plt.legend(frameon=False, ncol=2)\\n    else:\\n        plt.legend().remove()\\n    plt.xlabel(\\\"orientation\\\")\\n\\n\\n#     plt.ylabel(\\\"average stimulus\\\")\\n\\n\\ndef normalize_across_trial(m, get_first=False):\\n    if get_first:\\n        return m[0] - m[0].mean(0)\\n    else:\\n        return m - m.mean(0)\\n\\n\\ndef eigsorted(cov):\\n    vals, vecs = np.linalg.eigh(cov)\\n    order = vals.argsort()[::-1]\\n    return vals[order], vecs[:, order]\\n\\n\\ndef draw_ellipse(x, y, ax, label, color):\\n    nstd = 2\\n    cov = np.cov(x, y)\\n    vals, vecs = eigsorted(cov)\\n    theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\\n    w, h = 2 * nstd * np.sqrt(vals)\\n    sc = plt.scatter(x, y, label=label, s=3, c=color)\\n    ell = Ellipse(\\n        xy=(np.mean(x), np.mean(y)),\\n        width=w,\\n        height=h,\\n        angle=theta,\\n        #         color=sc.get_facecolors()[0].tolist(),\\n        color=color,\\n        linestyle=\\\"-\\\",\\n    )\\n\\n    ell.set_facecolor(\\\"none\\\")\\n    xmax, ymax = ell.get_patch_transform().transform(ell.get_path().vertices).max(0)\\n    xmin, ymin = ell.get_patch_transform().transform(ell.get_path().vertices).min(0)\\n    ax.add_artist(ell)\\n    ax.axes.xaxis.set_major_locator(plt.MaxNLocator(4))\\n    ax.axes.yaxis.set_major_locator(plt.MaxNLocator(4))\\n    return xmin, ymin, xmax, ymax\\n\\n\\ndef filter_data_sample(data_sample, save=False):\\n    if save:\\n        mask = np.array(\\n            [True if i >= 210 and i <= 330 else False for i in data_sample[-3][0]],\\n            dtype=bool,\\n        ) & np.array((data_sample[0] > 200).sum(1) == 0, dtype=bool)\\n        data_sample[0] = (data_sample[0][mask],)\\n        data_sample[1] = (data_sample[1][mask],)\\n        data_sample[2] = (data_sample[2][:, mask],)\\n        data_sample[3] = (data_sample[3][mask],)\\n        data_sample[-3] = (data_sample[-3][:, mask],)\\n        data_sample[-2] = (data_sample[-2],)\\n        data_sample[-1] = (data_sample[-1][:, mask],)\\n    else:\\n        mask = np.array(\\n            [True if i >= 210 and i <= 330 else False for i in data_sample[1][0]],\\n            dtype=bool,\\n        ) & np.array((data_sample[0] > 200).sum(1) == 0, dtype=bool)\\n        data_sample[0] = data_sample[0][mask, :]\\n        data_sample[1] = data_sample[1][:, mask]\\n    return data_sample\\n\\n\\ndef get_index_list(pair_neuron_list):\\n    l = []\\n    for i in pair_neuron_list:\\n        ref_df = pd.DataFrame(np.concatenate(np.triu_indices(96)).reshape(2, -1)).T\\n        l.append(ref_df[(ref_df[0] == i[0]) & (ref_df[1] == i[1])].index[0])\\n    return l\\n\\n\\ndef sensivity(x, y):\\n    return (x.mean(0) - y.mean(0)) / np.where(\\n        ((np.var(x, 0) + np.var(y, 0)) / 2) ** 0.5 == 0,\\n        np.nan,\\n        ((np.var(x, 0) + np.var(y, 0)) / 2) ** 0.5,\\n    )\\n\\n\\n\\\"\\\"\\\"\\ncalculate sensitivity\\n\\\"\\\"\\\"\\n\\n\\ndef all_sensitivity(data, savename, monkey_num, bz=3, cate=\\\"linear\\\"):\\n    sens_total = []\\n    for d in data:\\n        data_sample = filter_data_sample(d)\\n        dfs = split_data(data_sample, bin_size=bz)\\n        if cate in [\\\"cross\\\", \\\"square\\\"]:\\n            norm_dfs = [normalize_across_trial(i) for i in list(dfs.values())]\\n            r_ij = np.expand_dims(np.concatenate(norm_dfs), 2) * np.expand_dims(\\n                np.concatenate(norm_dfs), 1\\n            )\\n            if cate == \\\"cross\\\":\\n                sep_index = np.triu_indices(96, k=1)\\n            else:\\n                sep_index = np.diag_indices(96)\\n            r_ij_flat = np.vstack([r_ij[i][sep_index] for i in range(r_ij.shape[0])])\\n            data_sample = filter_data_sample([r_ij_flat, data_sample[1]])\\n            dfs = split_data(data_sample, bin_size=bz)\\n        value_list = list(dfs.values())\\n\\n        sens_total.extend(\\n            np.concatenate(\\n                [\\n                    sensivity(value_list[i + 1], value_list[i])\\n                    for i in range(len(dfs) - 1)\\n                    if value_list[i].shape[0] >= 10 and value_list[i + 1].shape[0] >= 10\\n                ]\\n            )\\n        )\\n\\n    pd.Series(sens_total).hist(bins=np.linspace(-2, 2, 100))\\n\\n    plt.title(\\\"monkey \\\" + monkey_num + \\\" all sessions, \\\" + str(cate))\\n    plt.grid(False)\\n    plt.savefig(savename + \\\"_filter.pdf\\\")\\n\\n\\ndef single_sensitivity(data, session_num, savename, bz=3, cate=\\\"linear\\\", save=True):\\n    sens_total = []\\n    data_sample = filter_data_sample(data[session_num])\\n    dfs = split_data(data_sample, bin_size=bz)\\n    if cate in [\\\"cross\\\", \\\"square\\\"]:\\n        norm_dfs = [normalize_across_trial(i) for i in list(dfs.values())]\\n        r_ij = np.expand_dims(np.concatenate(norm_dfs), 2) * np.expand_dims(\\n            np.concatenate(norm_dfs), 1\\n        )\\n        if cate == \\\"cross\\\":\\n            sep_index = np.triu_indices(96, k=1)\\n        else:\\n            sep_index = np.diag_indices(96)\\n        r_ij_flat = np.vstack([r_ij[i][sep_index] for i in range(r_ij.shape[0])])\\n        data_sample = filter_data_sample([r_ij_flat, data_sample[1]])\\n        dfs = split_data(data_sample, bin_size=bz)\\n    value_list = list(dfs.values())\\n    sens_total.extend(\\n        [\\n            [\\n                sensivity(value_list[i + 1], value_list[i]),\\n                list(dfs.keys())[i].mid,\\n                list(dfs.keys())[i + 1].mid,\\n            ]\\n            for i in range(len(dfs) - 1)\\n            if value_list[i].shape[0] >= 10 and value_list[i + 1].shape[0] >= 10\\n        ]\\n    )\\n    edge = int((len(sens_total) - 4) // 2)\\n    for s in sens_total[edge : edge + 4]:\\n        pd.Series(s[0]).hist(\\n            bins=np.linspace(-1, 1, 50),\\n            alpha=0.7,\\n            label=\\\"orientation \\\" + str(s[1]) + \\\" \\\" + str(s[2]),\\n        )\\n    plt.legend()\\n    plt.title(\\n        \\\"monkey \\\"\\n        + str(monkey_num)\\n        + \\\", session: \\\"\\n        + str(session_num)\\n        + \\\", bin size: \\\"\\n        + str(bz)\\n        + \\\", \\\"\\n        + str(cate)\\n    )\\n    plt.grid(False)\\n    if save:\\n        plt.savefig(savename + \\\"_filter.pdf\\\")\\n\\n\\ndef change_dtypes(aa):\\n    revise_dtypes = {\\n        \\\"Counts_matrix\\\": \\\"uint8\\\",\\n        \\\"orientation\\\": \\\"double\\\",\\n        \\\"stimulus_class\\\": \\\"<U1\\\",\\n        \\\"trial_num\\\": \\\"uint16\\\",\\n        \\\"selected_class\\\": \\\"<U1\\\",\\n        \\\"session_number\\\": \\\"int\\\",\\n        \\\"contrast\\\": \\\"double\\\",\\n    }\\n    for key, item in aa.items():\\n        if key == \\\"session_number\\\":\\n            aa[key] = item[0]\\n        elif key in [\\\"trial_num\\\", \\\"contrast\\\", \\\"orientation\\\"]:\\n            aa[key] = item.reshape(1, -1).astype(revise_dtypes[key])\\n        else:\\n            aa[key] = item.astype(revise_dtypes[key])\\n    return aa\\n\\n\\nparams = {\\n    \\\"legend.fontsize\\\": 14,\\n    \\\"legend.frameon\\\": False,\\n    \\\"ytick.labelsize\\\": 14,\\n    \\\"xtick.labelsize\\\": 14,\\n    \\\"figure.dpi\\\": 300,\\n    \\\"axes.labelsize\\\": 14,\\n    \\\"axes.titlesize\\\": 14,\\n    \\\"pdf.fonttype\\\": 42,\\n    \\\"font.sans-serif\\\": \\\"Myriad Pro\\\",\\n    \\\"font.family\\\": \\\"sans-serif\\\",\\n}\\nplt.rcParams.update(params)\\n\\n\\nmonkey_num = \\\"2\\\"\\nbz = 3  # bin size is 3\\nsession_num = 20\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext nb_black\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "\n",
    "from scipy.io import loadmat, savemat\n",
    "from scipy.stats import f_oneway\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "\"\"\"\n",
    "monkey1\n",
    "59 sessions\n",
    "7 variablesï¼š(2380, 96)- spike count matrix, (2380,), (1, 2380), (2380,), (1, 2380)-orientation, (1, 1), (1, 2380)\n",
    "\n",
    "monkey2\n",
    "71 sessions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"sample to make the two monkey has comparable data size\"\"\"\n",
    "\n",
    "\n",
    "def sample_sessions(monkey_num, save=False):\n",
    "    data = loadmat(\"monkey\" + \"1\" + \".mat\")[\"monkey\" + \"1\"][0]\n",
    "    result = []\n",
    "    for i in data:\n",
    "        mask = i[0][0][-1][0] == max(i[0][0][-1][0])\n",
    "        result.append([i[0][0][0][mask], i[0][0][-3][0][mask].reshape(1, -1)])\n",
    "    samples = [\n",
    "        filter_data_sample(result[session_num])[0].shape[0]\n",
    "        for session_num in range(len(result))\n",
    "    ]\n",
    "\n",
    "    \"sample from normal distribution\"\n",
    "    while True:\n",
    "        norm_samples = np.random.normal(\n",
    "            loc=np.mean(samples), scale=np.std(samples), size=len(get_data(monkey_num))\n",
    "        ).round(0)\n",
    "        if (norm_samples < 0).sum() == 0:\n",
    "            norm_samples.sort()\n",
    "            break\n",
    "\n",
    "    \"sampling\"\n",
    "    data = loadmat(\"monkey\" + monkey_num + \".mat\")[\"monkey\" + monkey_num][0]\n",
    "    result = []\n",
    "    for i in data:\n",
    "        mask = i[0][0][-1][0] == max(i[0][0][-1][0])\n",
    "        result.append([i[0][0][0][mask], i[0][0][-3][0][mask].reshape(1, -1)])\n",
    "    sample_s = pd.DataFrame(\n",
    "        [\n",
    "            filter_data_sample(result[session_num])[0].shape[0]\n",
    "            for session_num in range(len(result))\n",
    "        ],\n",
    "        columns=[\"data2_samples\"],\n",
    "    )\n",
    "    result = (\n",
    "        sample_s.sort_values(by=\"data2_samples\")\n",
    "        .assign(ideal_samples=norm_samples)\n",
    "        .apply(lambda x: min(x.values), 1)\n",
    "    )\n",
    "    return result.sort_index()\n",
    "\n",
    "\n",
    "def sample_from_one_session(session_data, sample_number, save=False):\n",
    "    sort_nums = np.arange(session_data[0].shape[0])\n",
    "    random.shuffle(sort_nums)\n",
    "    if save:\n",
    "        return [\n",
    "            session_data[0][sort_nums[: int(sample_number)]],\n",
    "            session_data[1][sort_nums[: int(sample_number)]],\n",
    "            session_data[2][:, sort_nums[: int(sample_number)]],\n",
    "            session_data[3][sort_nums[: int(sample_number)]],\n",
    "            session_data[4][:, sort_nums[: int(sample_number)]],\n",
    "            session_data[5],\n",
    "            session_data[6][:, sort_nums[: int(sample_number)]],\n",
    "        ]\n",
    "    else:\n",
    "        return [\n",
    "            session_data[0][sort_nums[: int(sample_number)]],\n",
    "            session_data[1][:, sort_nums[: int(sample_number)]],\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_data(monkey_index, bz=3, if_sample=False, filter_neurons=False):\n",
    "    \"\"\"\n",
    "    i: seesion index, 0-counts_matrix, -3 orientation\n",
    "    \"\"\"\n",
    "    data = loadmat(\"monkey\" + monkey_index + \".mat\")[\"monkey\" + monkey_index][0]\n",
    "    result = []\n",
    "    for i in data:\n",
    "        mask = i[0][0][-1][0] == max(i[0][0][-1][0])\n",
    "        result.append([i[0][0][0][mask], i[0][0][-3][0][mask].reshape(1, -1)])\n",
    "    if if_sample:\n",
    "        sample_numbers = sample_sessions(monkey_index)\n",
    "        result = [\n",
    "            sample_from_one_session(result[i], sample_numbers[i])\n",
    "            for i in range(len(sample_numbers))\n",
    "        ]\n",
    "    if filter_neurons:\n",
    "        result_final = []\n",
    "        for session_num in range(len(result)):\n",
    "            data_sample = filter_data_sample(result[session_num])  # select one session\n",
    "            pvalues = f_oneway(*list(split_data(data_sample, bin_size=bz).values()))[1]\n",
    "            result_final.append([data_sample[0][:, pvalues < 0.05], data_sample[1]])\n",
    "        result = result_final\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_data_save(monkey_index, bz=3, if_sample=False, filter_neurons=False):\n",
    "    \"\"\"\n",
    "    i: seesion index, 0-counts_matrix, -3 orientation\n",
    "    \"\"\"\n",
    "    data = loadmat(\"monkey\" + monkey_index + \".mat\")[\"monkey\" + monkey_index][0]\n",
    "    result = []\n",
    "    for i in data:\n",
    "        mask = i[0][0][-1][0] == max(i[0][0][-1][0])\n",
    "        result.append(\n",
    "            [\n",
    "                i[0][0][0][mask],\n",
    "                i[0][0][1][mask],\n",
    "                i[0][0][2][:, mask],\n",
    "                i[0][0][3][mask],\n",
    "                i[0][0][-3][:, mask],\n",
    "                i[0][0][-2],\n",
    "                i[0][0][-1][:, mask],\n",
    "            ]\n",
    "        )\n",
    "    if if_sample:\n",
    "        sample_numbers = sample_sessions(monkey_index, save=True)\n",
    "        result = [\n",
    "            sample_from_one_session(result[i], sample_numbers[i], save=True)\n",
    "            for i in range(len(sample_numbers))\n",
    "        ]\n",
    "    if filter_neurons:\n",
    "        result_final = []\n",
    "        for session_num in range(len(result)):\n",
    "            data_sample = filter_data_sample(\n",
    "                result[session_num], save=True\n",
    "            )  # select one session\n",
    "            f_params = [\n",
    "                i[0]\n",
    "                for i in split_data(data_sample, bin_size=bz, save=True).values()\n",
    "                if len(i[0]) > 0\n",
    "            ]\n",
    "            pvalues = f_oneway(*f_params)[1]\n",
    "            result_final.append(\n",
    "                [\n",
    "                    data_sample[0][:, pvalues < 0.05],\n",
    "                    data_sample[1],\n",
    "                    data_sample[2],\n",
    "                    data_sample[3],\n",
    "                    data_sample[4],\n",
    "                    data_sample[5],\n",
    "                    data_sample[6],\n",
    "                ]\n",
    "            )\n",
    "        result = result_final\n",
    "    return result\n",
    "\n",
    "\n",
    "def split_data(data_sample, bin_size, limit=0, save=False):\n",
    "    #     \"Counts_matrix\",\n",
    "    #                             \"orientation\",\n",
    "    #                             \"stimulus_class\",\n",
    "    #                             \"trial_num\",\n",
    "    #                             \"selected_class\",\n",
    "    #                             \"session_number\",\n",
    "    #                             \"contrast\"\n",
    "    if save:\n",
    "        orientation = data_sample[-3].ravel()\n",
    "    else:\n",
    "        orientation = data_sample[1].ravel()\n",
    "    groups = pd.cut(\n",
    "        pd.Series(orientation),\n",
    "        range(\n",
    "            np.floor(orientation.min()).astype(int) - 1,\n",
    "            np.ceil(orientation.max()).astype(int) + bin_size,\n",
    "            bin_size,\n",
    "        ),\n",
    "    )\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(data_sample[0]),\n",
    "            pd.Series(orientation).rename(\"orientation\"),\n",
    "            groups.rename(\"group\"),\n",
    "        ],\n",
    "        1,\n",
    "    )\n",
    "    if save:\n",
    "        df = pd.concat(\n",
    "            [\n",
    "                df,\n",
    "                pd.Series(data_sample[1]).rename(\"stimulus_class\"),\n",
    "                pd.Series(data_sample[2].ravel()).rename(\"trial_num\"),\n",
    "                pd.Series(data_sample[3]).rename(\"selected_class\"),\n",
    "                pd.Series(list(data_sample[5][0]) * len(data_sample[1])).rename(\n",
    "                    \"session_number\"\n",
    "                ),\n",
    "                pd.Series(data_sample[6].ravel()).rename(\"contrast\"),\n",
    "            ],\n",
    "            1,\n",
    "        )\n",
    "    return dict(\n",
    "        df.groupby(\"group\")\n",
    "        .apply(\n",
    "            lambda x: [x.drop(\"group\", 1).values[:, :]]\n",
    "            #             + [\n",
    "            #                 each.ravel()\n",
    "            #                 for each in np.split(x.drop(\"group\", 1).values[:, -6:], range(1, 6), 1)\n",
    "            #             ]\n",
    "            #             if x.shape[0] >= limit\n",
    "            #             else np.nan\n",
    "        )\n",
    "        .dropna()\n",
    "    )\n",
    "\n",
    "\n",
    "def bootstrap_sample(x, size_per_time, exp_num=1, times=100):\n",
    "    sample = [\n",
    "        (np.random.choice(x, size_per_time) ** exp_num).mean() for _ in range(times)\n",
    "    ]\n",
    "    return [np.mean(sample), np.std(sample)]\n",
    "\n",
    "\n",
    "def plot_ten_neurons(dfs, select_p, exp_num=1, legend_on=True):\n",
    "    x = [i.mid for i in list(dfs.keys()) if dfs[i].shape[0] > 0]\n",
    "    for idx in select_p:\n",
    "        y, yerr = zip(\n",
    "            *[\n",
    "                bootstrap_sample(i[:, idx], i.shape[0], exp_num)\n",
    "                for i in list(dfs.values())\n",
    "            ]\n",
    "        )\n",
    "        if list(dfs.values())[0].shape[1] < 100:\n",
    "            if exp_num > 1:\n",
    "                plt.plot(x, y)\n",
    "                plt.fill_between(\n",
    "                    x,\n",
    "                    np.array(y) + np.array(yerr),\n",
    "                    np.array(y) - np.array(yerr),\n",
    "                    alpha=0.2,\n",
    "                    #                     capsize=3,\n",
    "                    label=r\"$<r^\" + str(exp_num) + \"_{\" + str(idx + 1) + \"}|s>$\",\n",
    "                )\n",
    "            else:\n",
    "                plt.plot(\n",
    "                    x, y, label=r\"$<r^\" + str(exp_num) + \"_{\" + str(idx + 1) + \"}|s>$\",\n",
    "                )\n",
    "                plt.fill_between(\n",
    "                    x,\n",
    "                    np.array(y) + np.array(yerr),\n",
    "                    np.array(y) - np.array(yerr),\n",
    "                    alpha=0.2,\n",
    "                    #                     capsize=3,\n",
    "                    #                     label=r\"$<r_{\" + str(idx + 1) + \"}|s>$\",\n",
    "                )\n",
    "        else:\n",
    "            triu_index = np.triu_indices(96)\n",
    "            plt.plot(x, y)\n",
    "            plt.fill_between(\n",
    "                x,\n",
    "                np.array(y) + np.array(yerr),\n",
    "                np.array(y) - np.array(yerr),\n",
    "                alpha=0.2,\n",
    "                #                     capsize=3,\n",
    "                label=r\"$<r_{\"\n",
    "                + str(triu_index[0][idx] + 1)\n",
    "                + \"} r_{\"\n",
    "                + str(triu_index[1][idx] + 1)\n",
    "                + \"}|s>$\",\n",
    "            )\n",
    "    if exp_num == 1 and legend_on == True:\n",
    "        plt.legend(frameon=False, ncol=2)\n",
    "    else:\n",
    "        plt.legend().remove()\n",
    "    plt.xlabel(\"orientation\")\n",
    "\n",
    "\n",
    "#     plt.ylabel(\"average stimulus\")\n",
    "\n",
    "\n",
    "def normalize_across_trial(m, get_first=False):\n",
    "    if get_first:\n",
    "        return m[0] - m[0].mean(0)\n",
    "    else:\n",
    "        return m - m.mean(0)\n",
    "\n",
    "\n",
    "def eigsorted(cov):\n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    order = vals.argsort()[::-1]\n",
    "    return vals[order], vecs[:, order]\n",
    "\n",
    "\n",
    "def draw_ellipse(x, y, ax, label, color):\n",
    "    nstd = 2\n",
    "    cov = np.cov(x, y)\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n",
    "    w, h = 2 * nstd * np.sqrt(vals)\n",
    "    sc = plt.scatter(x, y, label=label, s=3, c=color)\n",
    "    ell = Ellipse(\n",
    "        xy=(np.mean(x), np.mean(y)),\n",
    "        width=w,\n",
    "        height=h,\n",
    "        angle=theta,\n",
    "        #         color=sc.get_facecolors()[0].tolist(),\n",
    "        color=color,\n",
    "        linestyle=\"-\",\n",
    "    )\n",
    "\n",
    "    ell.set_facecolor(\"none\")\n",
    "    xmax, ymax = ell.get_patch_transform().transform(ell.get_path().vertices).max(0)\n",
    "    xmin, ymin = ell.get_patch_transform().transform(ell.get_path().vertices).min(0)\n",
    "    ax.add_artist(ell)\n",
    "    ax.axes.xaxis.set_major_locator(plt.MaxNLocator(4))\n",
    "    ax.axes.yaxis.set_major_locator(plt.MaxNLocator(4))\n",
    "    return xmin, ymin, xmax, ymax\n",
    "\n",
    "\n",
    "def filter_data_sample(data_sample, save=False):\n",
    "    if save:\n",
    "        mask = np.array(\n",
    "            [True if i >= 210 and i <= 330 else False for i in data_sample[-3][0]],\n",
    "            dtype=bool,\n",
    "        ) & np.array((data_sample[0] > 200).sum(1) == 0, dtype=bool)\n",
    "        data_sample[0] = (data_sample[0][mask],)\n",
    "        data_sample[1] = (data_sample[1][mask],)\n",
    "        data_sample[2] = (data_sample[2][:, mask],)\n",
    "        data_sample[3] = (data_sample[3][mask],)\n",
    "        data_sample[-3] = (data_sample[-3][:, mask],)\n",
    "        data_sample[-2] = (data_sample[-2],)\n",
    "        data_sample[-1] = (data_sample[-1][:, mask],)\n",
    "    else:\n",
    "        mask = np.array(\n",
    "            [True if i >= 210 and i <= 330 else False for i in data_sample[1][0]],\n",
    "            dtype=bool,\n",
    "        ) & np.array((data_sample[0] > 200).sum(1) == 0, dtype=bool)\n",
    "        data_sample[0] = data_sample[0][mask, :]\n",
    "        data_sample[1] = data_sample[1][:, mask]\n",
    "    return data_sample\n",
    "\n",
    "\n",
    "def get_index_list(pair_neuron_list):\n",
    "    l = []\n",
    "    for i in pair_neuron_list:\n",
    "        ref_df = pd.DataFrame(np.concatenate(np.triu_indices(96)).reshape(2, -1)).T\n",
    "        l.append(ref_df[(ref_df[0] == i[0]) & (ref_df[1] == i[1])].index[0])\n",
    "    return l\n",
    "\n",
    "\n",
    "def sensivity(x, y):\n",
    "    return (x.mean(0) - y.mean(0)) / np.where(\n",
    "        ((np.var(x, 0) + np.var(y, 0)) / 2) ** 0.5 == 0,\n",
    "        np.nan,\n",
    "        ((np.var(x, 0) + np.var(y, 0)) / 2) ** 0.5,\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "calculate sensitivity\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def all_sensitivity(data, savename, monkey_num, bz=3, cate=\"linear\"):\n",
    "    sens_total = []\n",
    "    for d in data:\n",
    "        data_sample = filter_data_sample(d)\n",
    "        dfs = split_data(data_sample, bin_size=bz)\n",
    "        if cate in [\"cross\", \"square\"]:\n",
    "            norm_dfs = [normalize_across_trial(i) for i in list(dfs.values())]\n",
    "            r_ij = np.expand_dims(np.concatenate(norm_dfs), 2) * np.expand_dims(\n",
    "                np.concatenate(norm_dfs), 1\n",
    "            )\n",
    "            if cate == \"cross\":\n",
    "                sep_index = np.triu_indices(96, k=1)\n",
    "            else:\n",
    "                sep_index = np.diag_indices(96)\n",
    "            r_ij_flat = np.vstack([r_ij[i][sep_index] for i in range(r_ij.shape[0])])\n",
    "            data_sample = filter_data_sample([r_ij_flat, data_sample[1]])\n",
    "            dfs = split_data(data_sample, bin_size=bz)\n",
    "        value_list = list(dfs.values())\n",
    "\n",
    "        sens_total.extend(\n",
    "            np.concatenate(\n",
    "                [\n",
    "                    sensivity(value_list[i + 1], value_list[i])\n",
    "                    for i in range(len(dfs) - 1)\n",
    "                    if value_list[i].shape[0] >= 10 and value_list[i + 1].shape[0] >= 10\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    pd.Series(sens_total).hist(bins=np.linspace(-2, 2, 100))\n",
    "\n",
    "    plt.title(\"monkey \" + monkey_num + \" all sessions, \" + str(cate))\n",
    "    plt.grid(False)\n",
    "    plt.savefig(savename + \"_filter.pdf\")\n",
    "\n",
    "\n",
    "def single_sensitivity(data, session_num, savename, bz=3, cate=\"linear\", save=True):\n",
    "    sens_total = []\n",
    "    data_sample = filter_data_sample(data[session_num])\n",
    "    dfs = split_data(data_sample, bin_size=bz)\n",
    "    if cate in [\"cross\", \"square\"]:\n",
    "        norm_dfs = [normalize_across_trial(i) for i in list(dfs.values())]\n",
    "        r_ij = np.expand_dims(np.concatenate(norm_dfs), 2) * np.expand_dims(\n",
    "            np.concatenate(norm_dfs), 1\n",
    "        )\n",
    "        if cate == \"cross\":\n",
    "            sep_index = np.triu_indices(96, k=1)\n",
    "        else:\n",
    "            sep_index = np.diag_indices(96)\n",
    "        r_ij_flat = np.vstack([r_ij[i][sep_index] for i in range(r_ij.shape[0])])\n",
    "        data_sample = filter_data_sample([r_ij_flat, data_sample[1]])\n",
    "        dfs = split_data(data_sample, bin_size=bz)\n",
    "    value_list = list(dfs.values())\n",
    "    sens_total.extend(\n",
    "        [\n",
    "            [\n",
    "                sensivity(value_list[i + 1], value_list[i]),\n",
    "                list(dfs.keys())[i].mid,\n",
    "                list(dfs.keys())[i + 1].mid,\n",
    "            ]\n",
    "            for i in range(len(dfs) - 1)\n",
    "            if value_list[i].shape[0] >= 10 and value_list[i + 1].shape[0] >= 10\n",
    "        ]\n",
    "    )\n",
    "    edge = int((len(sens_total) - 4) // 2)\n",
    "    for s in sens_total[edge : edge + 4]:\n",
    "        pd.Series(s[0]).hist(\n",
    "            bins=np.linspace(-1, 1, 50),\n",
    "            alpha=0.7,\n",
    "            label=\"orientation \" + str(s[1]) + \" \" + str(s[2]),\n",
    "        )\n",
    "    plt.legend()\n",
    "    plt.title(\n",
    "        \"monkey \"\n",
    "        + str(monkey_num)\n",
    "        + \", session: \"\n",
    "        + str(session_num)\n",
    "        + \", bin size: \"\n",
    "        + str(bz)\n",
    "        + \", \"\n",
    "        + str(cate)\n",
    "    )\n",
    "    plt.grid(False)\n",
    "    if save:\n",
    "        plt.savefig(savename + \"_filter.pdf\")\n",
    "\n",
    "\n",
    "def change_dtypes(aa):\n",
    "    revise_dtypes = {\n",
    "        \"Counts_matrix\": \"uint8\",\n",
    "        \"orientation\": \"double\",\n",
    "        \"stimulus_class\": \"<U1\",\n",
    "        \"trial_num\": \"uint16\",\n",
    "        \"selected_class\": \"<U1\",\n",
    "        \"session_number\": \"int\",\n",
    "        \"contrast\": \"double\",\n",
    "    }\n",
    "    for key, item in aa.items():\n",
    "        if key == \"session_number\":\n",
    "            aa[key] = item[0]\n",
    "        elif key in [\"trial_num\", \"contrast\", \"orientation\"]:\n",
    "            aa[key] = item.reshape(1, -1).astype(revise_dtypes[key])\n",
    "        else:\n",
    "            aa[key] = item.astype(revise_dtypes[key])\n",
    "    return aa\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"legend.frameon\": False,\n",
    "    \"ytick.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 14,\n",
    "    \"figure.dpi\": 300,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"font.sans-serif\": \"Myriad Pro\",\n",
    "    \"font.family\": \"sans-serif\",\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "\n",
    "monkey_num = \"2\"\n",
    "bz = 3  # bin size is 3\n",
    "session_num = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_limit = 5\n",
    "filter_neurons = False\n",
    "data = get_data(monkey_num, if_sample=False, filter_neurons=filter_neurons)\n",
    "data_sample = filter_data_sample(data[session_num])  # select one session\n",
    "dfs = split_data(data_sample, bin_size=bz)  # split stimulus data (r) based on bin size\n",
    "dfs = {k: dfs[k][0] for k in dfs.keys()}\n",
    "norm_dfs = [\n",
    "    normalize_across_trial(i)\n",
    "    for i in list(dfs.values())\n",
    "    if normalize_across_trial(i).shape[0] >= sample_limit\n",
    "]\n",
    "unnorm_dfs = [i for i in list(dfs.values()) if i.shape[0] >= sample_limit]\n",
    "\n",
    "key_list = [k for k in list(dfs.keys()) if dfs[k].shape[0] >= sample_limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row 1 A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for monkey in [\"1\", \"2\"]:\n",
    "    pd.Series([filter_data_sample(i)[0].shape[0] for i in get_data(monkey)]).hist(\n",
    "            grid=False, bins=range(0, 2000, 100), alpha=0.7\n",
    "        )\n",
    "    print(\n",
    "        np.mean([filter_data_sample(i)[0].shape[0] for i in get_data(monkey)]),\n",
    "        np.std([filter_data_sample(i)[0].shape[0] for i in get_data(monkey)]),\n",
    "    )\n",
    "plt.xlabel(\"number of trials\")\n",
    "plt.ylabel(\"session count\")\n",
    "plt.legend([\"Monkey 1\", \"Monkey 2\"], frameon=False)\n",
    "# plt.savefig(\"session_num_hist_filter.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row 1 B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for monkey in [\"1\", \"2\"]:\n",
    "    plot_data = loadmat(\"monkey\" + monkey + \"_filter.mat\")[\"data\"][0]\n",
    "    pd.Series([i[0][0][0].shape[0] for i in plot_data]).hist(\n",
    "        grid=False, bins=range(0, 2000, 100), alpha=0.7\n",
    "    )\n",
    "    print(\n",
    "        np.mean([filter_data_sample(i)[0].shape[0] for i in get_data(monkey)]),\n",
    "        np.std([filter_data_sample(i)[0].shape[0] for i in get_data(monkey)]),\n",
    "    )\n",
    "plt.xlabel(\"effective neuron count\")\n",
    "plt.ylabel(\"session count\")\n",
    "plt.legend([\"Monkey 1\", \"Monkey 2\"], frameon=False)\n",
    "# plt.savefig(\"effective_neuron_cnt_filter.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row 2 A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bz = 3\n",
    "for monkey_num in [\"1\", \"2\"]:\n",
    "    data = get_data(monkey_num)\n",
    "    effective_list = []\n",
    "    for s in data:\n",
    "        data_sample = filter_data_sample(s)  # select session one\n",
    "        dfs = split_data(\n",
    "            data_sample, bin_size=bz\n",
    "        )  # split stimulus data (r) based on bin size\n",
    "        key_list = list(dfs.keys())\n",
    "\n",
    "        pvalues = f_oneway(*[i for i in list(dfs.values()) if i.shape[0] > 0])[1]\n",
    "        effective_neurons = sum(pvalues < 0.05)\n",
    "        effective_list.append(effective_neurons)\n",
    "    #     print(np.mean(effective_list), np.std(effective_list))\n",
    "    pd.Series(effective_list).hist(grid=False, bins=range(0, 100, 5), alpha=0.7)\n",
    "plt.legend([\"Monkey 1\", \"Monkey 2\"], frameon=False)\n",
    "plt.xlabel(\"effective neuron count\")\n",
    "plt.ylabel(\"session count\")\n",
    "# plt.savefig(\"effective_neuron_cnt.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row 2 B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bz = 3\n",
    "for monkey_num in [\"1\", \"2\"]:\n",
    "    #     data = get_data(monkey_num)\n",
    "    plot_data = loadmat(\"monkey\" + monkey_num + \"_filter.mat\")[\"data\"][0]\n",
    "    data = [[i[0][0][0], i[0][0][4]] for i in plot_data]\n",
    "    effective_list = []\n",
    "    for s in data:\n",
    "        data_sample = filter_data_sample(s)  # select session one\n",
    "        if data_sample[0].shape[0] == 0:\n",
    "            continue\n",
    "        dfs = split_data(\n",
    "            data_sample, bin_size=bz\n",
    "        )  # split stimulus data (r) based on bin size\n",
    "        key_list = list(dfs.keys())\n",
    "\n",
    "        pvalues = f_oneway(*[i for i in list(dfs.values()) if i.shape[0] > 0])[1]\n",
    "        effective_neurons = sum(pvalues < 0.05)\n",
    "        effective_list.append(effective_neurons)\n",
    "    #     print(np.mean(effective_list), np.std(effective_list))\n",
    "    pd.Series(effective_list).hist(grid=False, bins=range(0, 100, 5), alpha=0.7)\n",
    "plt.legend([\"Monkey 1\", \"Monkey 2\"], frameon=False)\n",
    "plt.xlabel(\"effective neuron count\")\n",
    "plt.ylabel(\"session count\")\n",
    "# plt.savefig(\"effective_neuron_cnt.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
